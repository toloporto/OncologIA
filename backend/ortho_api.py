
# backend/ortho_api.py

import os
import sys
import threading
import cv2
import numpy as np
# import tensorflow as tf # Moved to Lazy Load
# from tensorflow import keras # Moved to Lazy Load

from fastapi import FastAPI, File, UploadFile, HTTPException, Depends, Request, Form, Query
from fastapi.responses import JSONResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from sqlalchemy.orm import Session
from pydantic import BaseModel
from backend.deepseek_routes import router as deepseek_router


# Clase personalizada para FileResponse con CORS garantizado
class CORSFileResponse(FileResponse):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.headers["Access-Control-Allow-Origin"] = "*"
        self.headers["Access-Control-Allow-Methods"] = "GET, OPTIONS"
        self.headers["Access-Control-Allow-Headers"] = "*"
        self.headers["Access-Control-Expose-Headers"] = "*"
import datetime
import uuid
import json
import logging
from PIL import Image
import io
import base64
from typing import Dict, Any, List, Optional

# A√±adir el directorio ra√≠z del proyecto al sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Ahora podemos importar los m√≥dulos de la base de datos
from backend.database import SessionLocal, engine, get_db
from backend.models import Base, User, AnalysisResult, Patient
from backend.auth import create_access_token, verify_password, get_password_hash
from backend.auth_routes import auth_router
from backend.ipfs_routes import ipfs_router
from backend.generative_manager import GenerativeManager
from backend.explainability import GradCAM
from backend.cyclegan_service import cyclegan_service
from backend.landmarks_service import landmarks_service
from backend.services import PredictionService, AnalysisService, ModelNotAvailableError
from backend.file_validator import validate_upload_file, FileValidationError
from backend.rate_limiter import limiter, rate_limit_exceeded_handler, UPLOAD_RATE_AUTHENTICATED
from backend.services.selenium_service import selenium_service
from slowapi.errors import RateLimitExceeded

# Configuraci√≥n del logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Crear tablas en la base de datos
Base.metadata.create_all(bind=engine)

# --- Creaci√≥n de usuario de prueba ---
def create_test_user_if_not_exists():
    db = SessionLocal()
    try:
        test_user = db.query(User).filter(User.email == "test@ortho.com").first()
        if not test_user:
            hashed_password = get_password_hash("OrthoWeb3_Demo2024!")
            new_user = User(
                id=str(uuid.uuid4()),
                email="test@ortho.com",
                hashed_password=hashed_password,
                full_name="Usuario de Prueba",
                is_active=True
            )
            db.add(new_user)
            db.commit()
            logger.info("‚úÖ Usuario de prueba creado: test@ortho.com / OrthoWeb3_Demo2024!")
        else:
            logger.info("‚ÑπÔ∏è Usuario de prueba ya existe.")
    except Exception as e:
        logger.error(f"‚ùå Error creando usuario de prueba: {e}")
        db.rollback()
    finally:
        db.close()

# --- Carga de Modelos de IA ---
# Obtener la ruta base del proyecto (un nivel arriba de backend)
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
MODEL_PATH = os.path.join(BASE_DIR, 'ml-models', 'models', 'ortho_efficientnetv2.h5')
METRICS_PATH = os.path.join(BASE_DIR, 'ml-models', 'trained_models', 'real_ortho_model_metrics.json')
SEGMENTATION_MODEL_PATH = os.path.join(BASE_DIR, 'ml-models', 'trained_models', 'unet_dental_model.h5')
LANDMARKS_MODEL_PATH = os.path.join(BASE_DIR, 'ml-models', 'trained_models', 'shape_predictor_68_face_landmarks.dat')

# --- Gesti√≥n de Modelos (Lazy Loading) ---
class ModelManager:
    def __init__(self):
        self.model = None
        self.metrics = None
        self.segmentation_model = None
        self.landmarks_predictor = None
        self.generative_manager = None
        self._loading_lock = False # Simple flag, could be a real Lock if threaded

    def get_classification_model(self):
        if self.model is None:
            self._load_classification_model()
        return self.model

    def get_metrics(self):
        if self.metrics is None:
            self._load_metrics()
        return self.metrics

    def get_segmentation_model(self):
        if self.segmentation_model is None:
            self._load_segmentation_model()
        return self.segmentation_model

    def get_landmarks_predictor(self):
        if self.landmarks_predictor is None:
            self._load_landmarks_predictor()
        return self.landmarks_predictor

    def get_generative_manager(self):
        if self.generative_manager is None:
            self._load_generative_manager()
        return self.generative_manager

    def _load_classification_model(self):
        logger.info(f"üîÑ Cargando modelo de clasificaci√≥n (Lazy Load)...")
        # Lazy Import
        try:
            import tensorflow as tf
            from tensorflow import keras
        except ImportError:
            import keras

        if os.path.exists(MODEL_PATH):
            try:
                self.model = keras.models.load_model(MODEL_PATH, compile=False)
                self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
                logger.info("‚úÖ Modelo de clasificaci√≥n cargado correctamente")
            except Exception as e:
                logger.error(f"‚ùå Error al cargar modelo de clasificaci√≥n: {e}")
                self.model = None
        else:
            logger.warning(f"‚ö†Ô∏è Modelo no encontrado en: {MODEL_PATH}")
            self.model = None

    def _load_metrics(self):
        if os.path.exists(METRICS_PATH):
            try:
                with open(METRICS_PATH, 'r') as f:
                    self.metrics = json.load(f)
            except Exception as e:
                logger.error(f"‚ùå Error al cargar m√©tricas: {e}")
                self.metrics = None
        else:
            self.metrics = None

    def _load_segmentation_model(self):
        logger.info(f"üîÑ Cargando modelo de segmentaci√≥n (Lazy Load)...")
        # Lazy Import
        try:
            import tensorflow as tf
            from tensorflow import keras
        except ImportError:
            import keras

        if os.path.exists(SEGMENTATION_MODEL_PATH):
            try:
                self.segmentation_model = keras.models.load_model(SEGMENTATION_MODEL_PATH, compile=False)
                self.segmentation_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
                logger.info("‚úÖ Modelo de segmentaci√≥n cargado correctamente")
            except Exception as e:
                logger.error(f"‚ùå Error al cargar modelo de segmentaci√≥n: {e}")
                self.segmentation_model = None
        else:
            logger.warning(f"‚ö†Ô∏è Modelo de segmentaci√≥n no encontrado")
            self.segmentation_model = None

    def _load_landmarks_predictor(self):
        logger.info(f"üîÑ Cargando modelo de landmarks (Lazy Load)...")
        try:
            import dlib
            if os.path.exists(LANDMARKS_MODEL_PATH):
                self.landmarks_predictor = dlib.shape_predictor(LANDMARKS_MODEL_PATH)
                logger.info("‚úÖ Modelo de landmarks cargado correctamente")
            else:
                logger.warning(f"‚ö†Ô∏è Archivo de landmarks no encontrado")
                self.landmarks_predictor = None
        except ImportError:
            logger.warning("‚ö†Ô∏è Dlib no disponible")
            self.landmarks_predictor = None
        except Exception as e:
            logger.error(f"‚ùå Error cargando landmarks: {e}")
            self.landmarks_predictor = None

    def _load_generative_manager(self):
        logger.info(f"üîÑ Inicializando GenerativeManager (Lazy Load)...")
        try:
            self.generative_manager = GenerativeManager()
            logger.info("‚úÖ GenerativeManager inicializado")
        except Exception as e:
            logger.error(f"‚ùå Error inicializando GenerativeManager: {e}")
            self.generative_manager = None

# Instancia global del gestor de modelos
model_manager = ModelManager()

# Instancias de servicios (Service Layer)
prediction_service = PredictionService(model_manager)

# --- Inicializaci√≥n de Usuario de Prueba ---
create_test_user_if_not_exists()
# NOTA: Ya no llamamos a load_model() aqu√≠. Se cargar√°n bajo demanda.

# Crear directorio de im√°genes p√∫blicas si no existe
if not os.path.exists("public_images"):
    os.makedirs("public_images")
    logger.info("‚úÖ Directorio public_images creado")

# --- Configuraci√≥n de FastAPI ---
app = FastAPI(
    title="OrthoWeb3 API",
    description="API para an√°lisis de im√°genes dentales y gesti√≥n de datos con Web3",
    version="1.0.0"
)

@app.on_event("startup")
async def startup_event():
    logger.info("Ejecutando eventos de inicio...")
    selenium_service.start()

    # --- Calentamiento Opcional de Modelos de IA ---
    if os.getenv("CYCLEGAN_WARMUP_ON_STARTUP", "false").lower() in ("true", "1", "yes"):
        logger.info("üî• Se detect√≥ la variable de entorno para el calentamiento de CycleGAN.")
        # Usamos un thread para no bloquear el inicio del servidor
        warmup_thread = threading.Thread(target=cyclegan_service.warm_up, daemon=True)
        warmup_thread.start()

# Configurar rate limiter
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, rate_limit_exceeded_handler)

# Middleware personalizado para asegurar CORS en archivos est√°ticos
@app.middleware("http")
async def add_cors_header(request: Request, call_next):
    response = await call_next(request)
    # Forzar cabeceras CORS en TODAS las respuestas
    response.headers["Access-Control-Allow-Origin"] = "*"
    response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, DELETE, OPTIONS"
    response.headers["Access-Control-Allow-Headers"] = "*"
    response.headers["Access-Control-Expose-Headers"] = "*"
    
    # Log para debug
    if "/public_images/" in str(request.url):
        logger.info(f"üñºÔ∏è Sirviendo imagen: {request.url} - CORS headers a√±adidos")
    
    return response

# Configuraci√≥n de CORS est√°ndar (para preflight OPTIONS)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Incluir los routers de autenticaci√≥n e IPFS
# Incluir los routers de autenticaci√≥n e IPFS
app.include_router(auth_router, prefix="/auth", tags=["Autenticaci√≥n"])
app.include_router(ipfs_router, prefix="/ipfs", tags=["IPFS"])

app.include_router(deepseek_router)

# Tambi√©n puedes a√±adir un endpoint de bienvenida que muestre los servicios:
# @app.get("/")
# async def root():
#     return {
#         "app": "OrthoWeb3",
#         "version": "1.0.0",
#         "services": {
#             "deepseek_selenium": selenium_service.is_ready,
#             "endpoints": {
#                 "dental_analysis": "/api/deepseek/analyze-dental-selenium",
#                 "health_check": "/api/deepseek/selenium-health",
#                 "test": "/api/deepseek/test-dental"
#             }
#         }
#     }


# Montar archivos est√°ticos para la galer√≠a p√∫blica
# Endpoint manual para servir im√°genes con CORS expl√≠cito (Infalible)
@app.get("/public_images/{filename}", tags=["Galer√≠a de Demo"])
async def get_public_image(filename: str):
    logger.info(f"üì• Petici√≥n de imagen: {filename}")
    file_path = os.path.join("public_images", filename)
    if os.path.exists(file_path):
        logger.info(f"‚úÖ Archivo encontrado: {file_path}")
        # Usar CORSFileResponse que garantiza cabeceras CORS
        return CORSFileResponse(file_path)
    logger.error(f"‚ùå Archivo NO encontrado: {file_path}")
    raise HTTPException(status_code=404, detail="Imagen no encontrada")

# Endpoint OPTIONS para CORS preflight
@app.options("/public_images/{filename}")
async def options_public_image(filename: str):
    return JSONResponse(
        content={},
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET, OPTIONS",
            "Access-Control-Allow-Headers": "*"
        }
    )

# --- Dependencias de Seguridad (SoC) ---
from fastapi.security import OAuth2PasswordBearer
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="auth/login")

def get_current_user(token: str = Depends(oauth2_scheme), db: Session = Depends(get_db)):
    """
    Dependencia para obtener el usuario actual desde el token JWT.
    Centraliza la l√≥gica de autenticaci√≥n y manejo de errores 401.
    """
    from backend.auth import decode_token
    
    token_data = decode_token(token)
    if not token_data:
        raise HTTPException(
            status_code=401,
            detail="No se pudieron validar las credenciales",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    user = db.query(User).filter(User.email == token_data.email).first()
    if not user:
        raise HTTPException(status_code=401, detail="Usuario no encontrado")
        
    return user

def get_analysis_service(db: Session = Depends(get_db)):
    """Dependencia para obtener el servicio de an√°lisis"""
    return AnalysisService(prediction_service, db)

# --- Clases de Datos (Pydantic) ---
class AnalysisRequest(BaseModel):
    image: str  # La imagen en base64

class Recommendation(BaseModel):
    diagnosis: str
    recommendation: str
    urgency: str
    suggested_treatment: str
    confidence_note: str

class AnalysisResponse(BaseModel):
    success: bool
    analysis_id: str
    timestamp: str
    patient_did: str
    predicted_class: str
    confidence: float
    all_confidences: dict
    class_index: int
    recommendation: Recommendation
    segmentation_mask: str # M√°scara de segmentaci√≥n en base64

# --- Constantes y Mapeos ---
CLASS_NAMES = [
    "class_i_normal",
    "class_ii_division1",
    "class_ii_division2",
    "class_iii",
    "open_bite",
    "cross_bite"
]

RECOMMENDATIONS = {
    "class_i_normal": {
        "diagnosis": "Oclusi√≥n Normal (Clase I)",
        "recommendation": "Mantener higiene y revisiones peri√≥dicas.",
        "urgency": "baja",
        "suggested_treatment": "Profilaxis y seguimiento.",
        "confidence_note": "Alta confidencia en diagn√≥stico."
    },
    "class_ii_division1": {
        "diagnosis": "Maloclusi√≥n Clase II Divisi√≥n 1",
        "recommendation": "Requiere evaluaci√≥n ortod√≥ntica para posible correcci√≥n.",
        "urgency": "media",
        "suggested_treatment": "Ortodoncia, posible avance mandibular.",
        "confidence_note": "Alta confidencia en diagn√≥stico."
    },
    "class_ii_division2": {
        "diagnosis": "Maloclusi√≥n Clase II Divisi√≥n 2",
        "recommendation": "Consulta con ortodoncista para evaluar la retroinclinaci√≥n de incisivos.",
        "urgency": "media",
        "suggested_treatment": "Ortodoncia para corregir inclinaci√≥n y mordida.",
        "confidence_note": "Alta confidencia en diagn√≥stico."
    },
    "class_iii": {
        "diagnosis": "Maloclusi√≥n Clase III",
        "recommendation": "Evaluaci√≥n urgente por ortodoncista y posible cirujano maxilofacial.",
        "urgency": "alta",
        "suggested_treatment": "Ortodoncia y/o cirug√≠a ortogn√°tica.",
        "confidence_note": "Alta confidencia en diagn√≥stico."
    },
    "open_bite": {
        "diagnosis": "Mordida Abierta",
        "recommendation": "Evaluaci√≥n para determinar la causa (esquel√©tica o dental) y plan de tratamiento.",
        "urgency": "media-alta",
        "suggested_treatment": "Ortodoncia, possibly with TADs or surgery.",
        "confidence_note": "Alta confidencia en diagn√≥stico."
    },
    "cross_bite": {
        "diagnosis": "Mordida Cruzada",
        "recommendation": "Correcci√≥n temprana es a menudo recomendada para evitar problemas de desarrollo.",
        "urgency": "media",
        "suggested_treatment": "Expansi√≥n del paladar, ortodoncia.",
        "confidence_note": "Alta confidencia en diagn√≥stico."
    },
    "default": {
        "diagnosis": "Evaluaci√≥n requerida",
        "recommendation": "Consulta con especialista para diagn√≥stico completo.",
        "urgency": "media",
        "suggested_treatment": "Evaluaci√≥n cl√≠nica completa",
        "confidence_note": "Confidencia variable. Se necesita m√°s informaci√≥n."
    }
}

# --- Funciones de Procesamiento de Im√°genes ---
def preprocess_image(image_bytes: bytes, target_size=(512, 512)) -> np.ndarray:
    """Preprocesa una imagen para el modelo de clasificaci√≥n."""
    try:
        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
        image = image.resize(target_size)
        image_array = np.array(image)
        image_array = image_array / 255.0  # Normalizar
        return np.expand_dims(image_array, axis=0)
    except Exception as e:
        logger.error(f"Error al preprocesar imagen: {e}")
        raise

def preprocess_for_segmentation(image_bytes: bytes, target_size=(512, 512)) -> np.ndarray:
    """Preprocesa una imagen para el modelo de segmentaci√≥n."""
    try:
        image = Image.open(io.BytesIO(image_bytes)).convert('RGB') # Convertir a RGB (3 canales)
        image = image.resize(target_size)
        image_array = np.array(image)
        image_array = image_array / 255.0
        return np.expand_dims(image_array, axis=0) # A√±adir batch dim
    except Exception as e:
        logger.error(f"Error al preprocesar para segmentaci√≥n: {e}")
        raise

def postprocess_segmentation_mask(mask: np.ndarray, original_size) -> Image:
    """Post-procesa la m√°scara de segmentaci√≥n a una imagen visible."""
    mask = (mask * 255).astype(np.uint8)
    mask_image = Image.fromarray(mask)
    mask_image = mask_image.resize(original_size, Image.NEAREST)
    
    # Crear una imagen RGBA coloreada para la m√°scara
    mask_rgba = mask_image.convert('RGBA')
    data = np.array(mask_rgba)
    red, green, blue, alpha = data.T

    # √Åreas blancas (dientes) se vuelven verdes semi-transparentes
    white_areas = (red > 200) & (green > 200) & (blue > 200)
    data[...][white_areas.T] = (0, 204, 153, 150) # Tono verde-azulado semitransparente

    # √Åreas no-blancas (fondo) se vuelven transparentes
    black_areas = ~white_areas
    data[...][black_areas.T] = (0, 0, 0, 0)
    
    return Image.fromarray(data)


# --- Endpoints de la API ---
@app.get("/health", tags=["Sistema"])
def health_check():
    """Verifica que la API est√© funcionando."""
    # Con Lazy Loading, el modelo siempre est√° "listo para cargar", as√≠ que devolvemos True
    # para que el frontend no muestre error.
    return {
        "status": "ok",
        "model_loaded": True 
    }

@app.get("/model-info", tags=["Modelo"])
def get_model_info():
    """Devuelve informaci√≥n sobre el modelo cargado y sus m√©tricas."""
    # Aqu√≠ podr√≠amos decidir si cargar el modelo o solo mostrar info si ya est√° cargado
    # Para info completa, cargamos el modelo
    model = model_manager.get_classification_model()
    metrics = model_manager.get_metrics()
    
    if model is None:
        return {
            "model_loaded": False,
            "error": "Modelo no cargado",
            "class_count": 0,
            "class_names": []
        }
    
    info = {
        "model_loaded": True,
        "class_count": len(CLASS_NAMES),
        "class_names": CLASS_NAMES,
        "model_summary": [],
        "metrics": metrics or "No disponibles"
    }
    try:
        model.summary(print_fn=lambda x: info["model_summary"].append(x))
    except:
        pass
    return info

# --- Endpoints de Active Learning (Revisi√≥n M√©dica) ---

@app.get("/reviews/pending", tags=["Active Learning"])
async def get_pending_reviews(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Obtener lista de an√°lisis marcados para revisi√≥n por baja confianza"""
    from backend.models import AnalysisReview, AnalysisResult
    
    reviews = db.query(AnalysisReview).join(AnalysisResult).filter(
        AnalysisReview.status == "pending"
    ).all()
    
    result = []
    for r in reviews:
        analysis = db.query(AnalysisResult).filter(AnalysisResult.id == r.analysis_id).first()
        result.append({
            "id": r.id,
            "analysis_id": r.analysis_id,
            "image_url": f"/public_images/{analysis.image_filename}" if analysis else None,
            "predicted_class": analysis.predicted_class if analysis else None,
            "confidence": r.confidence_at_prediction,
            "created_at": r.created_at.isoformat()
        })
    return result

@app.post("/reviews/{review_id}/submit", tags=["Active Learning"])
async def submit_review(
    review_id: str,
    data: Dict[str, Any],
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Enviar correcci√≥n de un doctor para un an√°lisis dudosso"""
    from backend.services.active_learning_service import active_learning_service
    
    correct_label = data.get("correct_label")
    notes = data.get("notes")
    
    if not correct_label:
        raise HTTPException(status_code=400, detail="Debe proporcionar la etiqueta correcta")
        
    success = active_learning_service.submit_doctor_review(
        db, review_id, correct_label, notes
    )
    
    if success:
        return {"success": True, "message": "Feedback registrado correctamente"}
    raise HTTPException(status_code=404, detail="Revisi√≥n no encontrada o error al procesar")

@app.get("/reviews/{review_id}/explain", tags=["Active Learning", "XAI"])
async def explain_review(
    review_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """Generar Grad-CAM para una revisi√≥n existente para ayudar al doctor"""
    from backend.models import AnalysisReview, AnalysisResult
    
    review = db.query(AnalysisReview).filter(AnalysisReview.id == review_id).first()
    if not review:
        raise HTTPException(status_code=404, detail="Revisi√≥n no encontrada")
        
    analysis = db.query(AnalysisResult).filter(AnalysisResult.id == review.analysis_id).first()
    if not analysis:
        raise HTTPException(status_code=404, detail="An√°lisis asociado no encontrado")
        
    # Cargar imagen
    file_path = os.path.join("public_images", analysis.image_filename)
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="Archivo de imagen no encontrado")
        
    try:
        with open(file_path, "rb") as f:
            image_bytes = f.read()
            
        # Generar explicaci√≥n
        result = prediction_service.predict_with_explanation(
            image_bytes,
            include_explanation=True
        )
        
        explanation = result.get('explanation')
        if not explanation or not explanation.get('success'):
            raise HTTPException(status_code=500, detail="Error generando Grad-CAM")
            
        return {
            "success": True,
            "explanation_image": explanation['heatmap_base64'],
            "predicted_class": analysis.predicted_class,
            "confidence": analysis.confidence
        }
    except Exception as e:
        logger.error(f"Error en explain_review: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/analyze", response_model=AnalysisResponse, tags=["An√°lisis"])

@limiter.limit(UPLOAD_RATE_AUTHENTICATED)
def analyze_image(
    request: Request,
    patient_did: str,
    file: UploadFile = File(...),
    use_ensemble: bool = Query(False, description="Usar ensemble de modelos"),
    current_user: User = Depends(get_current_user),
    analysis_service: AnalysisService = Depends(get_analysis_service)
):
    """
    Analiza una imagen dental para clasificar la oclusi√≥n y detectar problemas.
    Guarda el resultado en la base de datos.
    """
    try:
        # Leer imagen (en def s√≠ncrono, usamos file.file.read() o esperamos)
        # Para mantener compatibilidad con UploadFile sincronizado
        image_bytes = file.file.read()
        
        # Validar archivo
        try:
            safe_filename, mime_type = validate_upload_file(image_bytes, file.filename)
            logger.info(f"‚úÖ Archivo validado: {safe_filename} ({mime_type})")
        except FileValidationError as e:
            logger.warning(f"‚ö†Ô∏è Archivo rechazado en /analyze: {e}")
            raise HTTPException(status_code=400, detail=str(e))
        
        # Delegar al servicio
        result = analysis_service.analyze_dental_image(
            image_bytes=image_bytes,
            patient_did=patient_did,
            user_id=current_user.id,
            filename=safe_filename,
            use_ensemble=use_ensemble
        )
        
        # Construir respuesta
        response_data = {
            "success": True,
            "timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat(),
            "patient_did": patient_did,
            "segmentation_mask": None,  # Deshabilitado temporalmente
            "segmentation": {"mask": None, "polygons": []},
            **result
        }
        
        return JSONResponse(content=response_data)
        
    except ModelNotAvailableError as e:
        logger.error(f"Modelo no disponible: {e}")
        raise HTTPException(status_code=503, detail=str(e))
    except Exception as e:
        logger.error(f"‚ùå Error durante el an√°lisis: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Ocurri√≥ un error inesperado: {e}")

@app.post("/analyze/gallery", response_model=AnalysisResponse, tags=["An√°lisis"])
def analyze_gallery_image(
    patient_did: str,
    filename: str,
    use_ensemble: bool = Query(False, description="Usar ensemble de modelos"),
    current_user: User = Depends(get_current_user),
    analysis_service: AnalysisService = Depends(get_analysis_service)
):
    """
    Analiza una imagen existente en la galer√≠a del servidor.
    """
    # Buscar la imagen en public_images
    file_path = os.path.join("public_images", filename)
    if not os.path.exists(file_path):
        # Intentar extraer solo el nombre si viene una URL completa
        filename = os.path.basename(filename)
        file_path = os.path.join("public_images", filename)
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail="Imagen no encontrada en la galer√≠a")

    try:
        # Leer imagen del disco
        with open(file_path, "rb") as f:
            image_bytes = f.read()
        
        # Delegar al servicio (igual que /analyze)
        result = analysis_service.analyze_dental_image(
            image_bytes=image_bytes,
            patient_did=patient_did,
            user_id=current_user.id,
            filename=filename,
            use_ensemble=use_ensemble
        )
        
        # Construir respuesta
        response_data = {
            "success": True,
            "timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat(),
            "patient_did": patient_did,
            "segmentation_mask": None,
            "segmentation": {"mask": None, "polygons": []},
            **result
        }
        return JSONResponse(content=response_data)

    except ModelNotAvailableError as e:
        logger.error(f"Modelo no disponible: {e}")
        raise HTTPException(status_code=503, detail=str(e))
    except Exception as e:
        logger.error(f"‚ùå Error analizando imagen de galer√≠a: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error interno: {e}")

@app.post("/analyze/explain", tags=["An√°lisis", "XAI"])
@limiter.limit(UPLOAD_RATE_AUTHENTICATED)
def explain_analysis(
    request: Request,
    file: UploadFile = File(...),
    current_user: User = Depends(get_current_user)
):
    """
    Genera una explicaci√≥n visual (Grad-CAM) para la predicci√≥n del modelo.
    Devuelve la imagen con el mapa de calor superpuesto y regiones influyentes.
    """
    try:
        # Leer imagen
        image_bytes = file.file.read()
        
        # Validar archivo
        try:
            validate_upload_file(image_bytes, file.filename)
        except FileValidationError as e:
            logger.warning(f"‚ö†Ô∏è Archivo rechazado en /analyze/explain: {e}")
            raise HTTPException(status_code=400, detail=str(e))
        
        # Usar servicio para obtener predicci√≥n y explicaci√≥n
        result = prediction_service.predict_with_explanation(
            image_bytes,
            include_explanation=True
        )
        
        # Verificar si la explicaci√≥n fue generada
        explanation = result.get('explanation')
        if not explanation or not explanation.get('success'):
            raise HTTPException(
                status_code=500,
                detail="No se pudo generar la explicaci√≥n visual"
            )
        
        # Obtener clase predicha
        class_pred = result.get('class_pred')
        if class_pred is not None:
            class_idx = int(np.argmax(class_pred))
            predicted_class = CLASS_NAMES[class_idx]
        else:
            predicted_class = "unknown"
        
        return JSONResponse(content={
            "success": True,
            "predicted_class": predicted_class,
            "explanation_image": explanation['heatmap_base64'],
            "influential_regions": explanation['influential_regions'],
            "heatmap_entropy": explanation['heatmap_entropy'],
            "description": f"Mapa de calor mostrando las √°reas determinantes para la clase {predicted_class}"
        })
        
    except ModelNotAvailableError as e:
        logger.error(f"Modelo no disponible: {e}")
        raise HTTPException(status_code=503, detail=str(e))
    except Exception as e:
        logger.error(f"‚ùå Error generando explicaci√≥n Grad-CAM: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error generando explicaci√≥n: {e}")


@app.post("/simulate/treatment", tags=["Simulaci√≥n"])
@limiter.limit(UPLOAD_RATE_AUTHENTICATED)
def simulate_treatment(
    request: Request,
    file: UploadFile = File(...),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Genera una simulaci√≥n de tratamiento ortod√≥ntico usando CycleGAN.
    Transforma una imagen de dientes desalineados a dientes alineados.
    """
    # (Autenticaci√≥n manejada por Depends)
    
    # Validar archivo primero
    image_bytes = file.file.read()
    try:
        validate_upload_file(image_bytes, file.filename)
    except FileValidationError as e:
        logger.warning(f"‚ö†Ô∏è Archivo rechazado en /simulate/treatment: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    
    # Verificar que el servicio CycleGAN est√© disponible
    # Nota: cyclegan_service es un m√≥dulo importado, si queremos lazy load completo deber√≠amos
    # mover su inicializaci√≥n dentro de una funci√≥n o usar el ModelManager si lo integramos.
    # Por ahora, asumimos que cyclegan_service maneja su propia carga o es ligero.
    if not cyclegan_service.is_available():
        raise HTTPException(
            status_code=503, 
            detail="Servicio de simulaci√≥n no disponible. El modelo CycleGAN no est√° cargado."
        )
    
    try:
        logger.info("üé® Generando simulaci√≥n de tratamiento con CycleGAN...")
        
        # Generar simulaci√≥n
        simulated_image_bytes = cyclegan_service.generate_treatment_simulation(image_bytes)
        
        # Convertir a base64 para respuesta
        img_b64 = base64.b64encode(simulated_image_bytes).decode('utf-8')
        
        return JSONResponse(content={
            "success": True,
            "simulated_image": f"data:image/jpeg;base64,{img_b64}",
            "description": "Simulaci√≥n de tratamiento ortod√≥ntico generada con CycleGAN",
            "note": "Esta es una simulaci√≥n aproximada. Los resultados reales pueden variar."
        })
    
    except Exception as e:
        logger.error(f"‚ùå Error generando simulaci√≥n: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error generando simulaci√≥n: {e}")

@app.post("/analyze/landmarks", tags=["An√°lisis"])
@limiter.limit(UPLOAD_RATE_AUTHENTICATED)
def analyze_landmarks(
    request: Request, 
    file: UploadFile = File(...),
    current_user: User = Depends(get_current_user)
):
    """
    Detecta puntos faciales y dentales (Landmarks) en la imagen usando MediaPipe.
    Retorna 468 puntos faciales con coordenadas y m√©tricas calculadas.
    """
    try:
        logger.info("üîç Analizando landmarks faciales...")
        image_bytes = file.file.read()
        
        # Validar archivo
        try:
            validate_upload_file(image_bytes, file.filename)
        except FileValidationError as e:
            logger.warning(f"‚ö†Ô∏è Archivo rechazado en /analyze/landmarks: {e}")
            raise HTTPException(status_code=400, detail=str(e))
        
        # Opci√≥n r√°pida: Asegurarnos de que el predictor est√© cargado en el manager
        _ = model_manager.get_landmarks_predictor()
        
        result = landmarks_service.process_image(image_bytes)
        
        if not result:
            return JSONResponse(content={
                "success": False, 
                "message": "No se detect√≥ ning√∫n rostro en la imagen"
            })
            
        logger.info(f"‚úÖ Detectados {result['total_landmarks']} landmarks")
        return JSONResponse(content={
            "success": True,
            "data": result
        })
        
    except Exception as e:
        logger.error(f"‚ùå Error analizando landmarks: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error al analizar landmarks: {str(e)}")

@app.get("/history", tags=["An√°lisis"])
def get_analysis_history(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Devuelve el historial de an√°lisis para el usuario autenticado.
    """
    history = db.query(AnalysisResult).filter(AnalysisResult.user_id == current_user.id).order_by(AnalysisResult.timestamp.desc()).all()
    
    # El objeto history no es directamente serializable a JSON si contiene relaciones complejas.
    # Creamos una lista de diccionarios para asegurar la serializaci√≥n.
    history_list = []
    for record in history:
        # Robustez en la deserializaci√≥n de JSON para evitar fallos catastr√≥ficos si hay datos corruptos
        try:
            rec_data = json.loads(record.recommendation) if record.recommendation else None
        except Exception:
            rec_data = {"diagnosis": "Error", "recommendation": "Dato corrupto en DB"}
            
        try:
            conf_data = json.loads(record.all_confidences_json) if record.all_confidences_json else None
        except Exception:
            conf_data = {}

        history_list.append({
            "id": record.id,
            "user_id": record.user_id,
            "patient_did": record.patient_did,
            "image_filename": record.image_filename,
            "predicted_class": record.predicted_class,
            "confidence": record.confidence,
            "timestamp": record.timestamp.isoformat(),
            "recommendation": rec_data,
            "all_confidences": conf_data
        })
        
    return JSONResponse(content=history_list)

@app.get("/gallery/images", tags=["Galer√≠a de Demo"])
def get_gallery_images(request: Request):
    """
    Devuelve una lista de im√°genes de demostraci√≥n para la galer√≠a.
    Esta es una funci√≥n de ejemplo y deber√≠a ser reemplazada por una gesti√≥n de archivos real.
    """
    image_dir = "public_images"
    if not os.path.exists(image_dir):
        os.makedirs(image_dir)
        # Aqu√≠ podr√≠as a√±adir l√≥gica para copiar im√°genes de demo si no existen
    
    try:
        files = os.listdir(image_dir)
        image_files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        return image_files
    except Exception as e:
        logger.error(f"Error listando galer√≠a: {e}")
        return []

        return []

@app.get("/patients/{patient_did}/evolution", tags=["An√°lisis Temporal"])
async def get_patient_evolution(
    patient_did: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Obtiene la evoluci√≥n temporal del paciente basada en su historial de an√°lisis.
    Devuelve m√©tricas de severidad, gr√°fica de tendencia y proyecciones.
    """
    from backend.services.temporal_service import temporal_service
    
    # 1. Recuperar historial del paciente
    # Nota: Filtramos por DID de paciente, no solo por el usuario que consulta
    history = db.query(AnalysisResult).filter(
        AnalysisResult.patient_did == patient_did
    ).all()
    
    # 2. Verificar permisos (opcional: solo el doctor o el paciente mismo)
    # Por ahora dejamos abierto a usuarios autenticados
    
    # 3. Calcular evoluci√≥n
    try:
        evolution_data = temporal_service.analyze_progress(history)
        
        # Eliminar objetos datetime no serializables del timeline antes de responder
        for item in evolution_data.get("timeline", []):
            if "timestamp" in item:
                del item["timestamp"]
                
        return JSONResponse(content={
            "success": True,
            "patient_did": patient_did,
            "data": evolution_data
        })
    except Exception as e:
        logger.error(f"Error calculando evoluci√≥n: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error interno: {str(e)}")

@app.get("/patients/{patient_id}/evolution-check", tags=["An√°lisis Temporal"])
async def check_patient_evolution(
    patient_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Verifica la evoluci√≥n del paciente comparando los dos √∫ltimos an√°lisis.
    Genera una ALERTA si la mejora es inferior al 5% respecto al anterior.
    """
    try:
        # 1. Recuperar los dos √∫ltimos registros para ese patient_id (DID)
        history = db.query(AnalysisResult).filter(
            AnalysisResult.patient_did == patient_id
        ).order_by(AnalysisResult.timestamp.desc()).limit(2).all()
        
        if len(history) < 2:
            return JSONResponse(content={
                "status": "UNKNOWN",
                "patient_id": patient_id,
                "detail": "No hay suficientes datos hist√≥ricos (m√≠nimo 2 requeridos) para comparar."
            })
        
        current = history[0]
        previous = history[1]
        
        # 2. L√≥gica de Comparaci√≥n (usando confidence como m√©trica simple)
        # Improvement = (Current - Previous) / Previous
        curr_conf = current.confidence if current.confidence is not None else 0.0
        prev_conf = previous.confidence if previous.confidence is not None else 0.0001
        
        # Evitar divisi√≥n por cero si prev_conf es 0
        if prev_conf == 0:
            prev_conf = 0.0001
            
        improvement_metric = (curr_conf - prev_conf) / prev_conf
        
        # Si la mejora es inferior al 5% (0.05), marcar ALERTA.
        # Nota: Si improvement es negativo (empeor√≥), tambi√©n es < 0.05, por lo tanto ALERTA.
        status = "NORMAL"
        if improvement_metric < 0.05:
            status = "ALERTA"
            
        return JSONResponse(content={
            "status": status,
            "patient_id": patient_id,
            "improvement_metric": round(improvement_metric, 4),
            "improvement_percent": f"{improvement_metric*100:.2f}%",
            "current_confidence": curr_conf,
            "previous_confidence": prev_conf,
            "timestamp_current": current.timestamp.isoformat(),
            "timestamp_previous": previous.timestamp.isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error en check_patient_evolution: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error interno al verificar evoluci√≥n: {str(e)}")

@app.post("/analyze/multimodal", tags=["An√°lisis Avanzado"])
@limiter.limit(UPLOAD_RATE_AUTHENTICATED)
async def analyze_multimodal(
    request: Request,
    clinical_context: str = Form(...), # JSON string o lista separada por comas
    file: UploadFile = File(...),
    current_user: User = Depends(get_current_user)
):
    """
    An√°lisis Multimodal: Combina la imagen dental con contexto cl√≠nico (texto).
    Usa IA (CLIP) para determinar qu√© descripci√≥n cl√≠nica encaja mejor con la imagen.
    √ötil para validar s√≠ntomas reportados por el paciente.
    """
    from backend.services.multimodal_service import multimodal_service

    try:
        logger.info("üß† Iniciando an√°lisis multimodal...")
        
        # Procesar textos
        try:
            prompt_list = json.loads(clinical_context)
            if not isinstance(prompt_list, list):
                prompt_list = [str(prompt_list)]
        except:
            prompt_list = [t.strip() for t in clinical_context.split(',')]

        # Validar y leer imagen
        image_bytes = file.file.read()
        try:
            validate_upload_file(image_bytes, file.filename)
        except FileValidationError as e:
             raise HTTPException(status_code=400, detail=str(e))

        # Ejecutar an√°lisis
        result = multimodal_service.analyze_with_context(image_bytes, prompt_list)

        return JSONResponse(content={
            "success": True,
            "timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat(),
            "multimodal_result": result
        })

    except Exception as e:
        logger.error(f"‚ùå Error en endpoint multimodal: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error en an√°lisis multimodal: {str(e)}")



# --- Inicializaci√≥n Condicional de Generaci√≥n con IA ---
# Eliminado bloque global, ahora se maneja en ModelManager

class GenerativeRequest(BaseModel):
    prompt: str
    context: str # Puede ser el resultado de un an√°lisis, historial, etc.

@app.post("/simulate-treatment", tags=["Simulaci√≥n de Tratamiento"])
@limiter.limit(UPLOAD_RATE_AUTHENTICATED)
def simulate_treatment_generative(
    request: Request,
    treatment_type: str = "aligner",
    file: UploadFile = File(...),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Simula el resultado de un tratamiento ortod√≥ntico usando IA.
    Tipos disponibles: aligner, brackets, whitening
    """

    generative_manager = model_manager.get_generative_manager()
    if generative_manager is None:
        logger.error("Servicio de simulaci√≥n de tratamientos no disponible.")
        raise HTTPException(status_code=503, detail="Servicio de simulaciones no disponible. Verifique la instalaci√≥n de MediaPipe.")

    try:
        # Leer la imagen
        image_bytes = file.file.read()
        
        # Validar archivo
        try:
            validate_upload_file(image_bytes, file.filename)
        except FileValidationError as e:
            logger.warning(f"‚ö†Ô∏è Archivo rechazado en /simulate-treatment: {e}")
            raise HTTPException(status_code=400, detail=str(e))

        # Usar el gestor generativo para simular el tratamiento
        logger.info(f"Simulando tratamiento {treatment_type} con IA generativa")
        result = generative_manager.simulate_treatment(image_bytes, treatment_type)

        return JSONResponse(content=result)

    except Exception as e:
        logger.error(f"Error en simulaci√≥n de tratamiento: {e}")
        raise HTTPException(status_code=500, detail=f"Error en simulaci√≥n de tratamiento: {str(e)}")

@app.post("/generate/report", tags=["IA Generativa"])
async def generate_ia_report(
    request_body: GenerativeRequest, 
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Genera un informe o texto utilizando un modelo de lenguaje grande (LLM).
    """

    if generative_manager is None:
        logger.error("Servicio de generaci√≥n de texto no disponible.")
        raise HTTPException(status_code=503, detail="Servicio de IA generativa no disponible.")

    try:
        logger.info(f"Petici√≥n de generaci√≥n recibida con prompt: {request_body.prompt}")
        response = generative_manager.generate_text(request_body.prompt, request_body.context)
        return JSONResponse(content={"success": True, "generated_text": response})
    except Exception as e:
        logger.error(f"Error en la generaci√≥n de texto: {e}")
        raise HTTPException(status_code=500, detail="Error en el servicio de IA generativa.")


# --- Lanzador de la Aplicaci√≥n ---

# --- Servir Frontend (SPA) para Distribuci√≥n ---
# En modo producci√≥n/distribuci√≥n, el backend sirve los archivos est√°ticos de React
FRONTEND_DIST_DIR = os.path.join(BASE_DIR, "frontend", "dist")

if os.path.exists(FRONTEND_DIST_DIR):
    logger.info(f"üìÇ Sirviendo Frontend desde: {FRONTEND_DIST_DIR}")
    
    # 1. Montar assets est√°ticos (js, css, images)
    app.mount("/assets", StaticFiles(directory=os.path.join(FRONTEND_DIST_DIR, "assets")), name="assets")
    
    # 2. Servir index.html para la ruta ra√≠z y cualquier ruta no encontrada (SPA Fallback)
    @app.get("/{full_path:path}", include_in_schema=False)
    async def serve_spa(full_path: str):
        # Si la ruta comienza con /api, es un endpoint no encontrado -> 404 real
        if full_path.startswith("api/") or full_path.startswith("public_images/"):
            raise HTTPException(status_code=404, detail="Endpoint no encontrado")
            
        # Si es un archivo que existe en dist (ej. vite.svg), servirlo
        potential_file = os.path.join(FRONTEND_DIST_DIR, full_path)
        if os.path.exists(potential_file) and os.path.isfile(potential_file):
            return FileResponse(potential_file)
            
        # Para todo lo dem√°s (rutas de React Router), servir el index.html
        return FileResponse(os.path.join(FRONTEND_DIST_DIR, "index.html"))

else:
    logger.warning("‚ö†Ô∏è  Carpeta frontend/dist NO encontrada. La aplicaci√≥n web no se servir√° desde aqu√≠.")
    logger.warning("    Aseg√∫rate de ejecutar 'npm run build' en la carpeta frontend.")


# --- Lanzador de la Aplicaci√≥n ---
if __name__ == "__main__":
    import uvicorn
    logger.info("Iniciando servidor Uvicorn en http://0.0.0.0:8004")
    # Workers=1 es importante para PyInstaller y variables globales
    uvicorn.run(app, host="0.0.0.0", port=8004)

