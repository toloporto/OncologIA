import os
import logging
import datetime
from typing import Dict, Any, List
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from backend.services.rag_service import rag_service

logger = logging.getLogger(__name__)

class RiskAnalysis(BaseModel):
    risk_level: str = Field(description="Nivel de riesgo: 'low', 'medium', 'high', 'critical'")
    risk_found: bool = Field(description="Si se encontr√≥ riesgo de autolesi√≥n o peligro")
    explanation: str = Field(description="Explicaci√≥n breve del razonamiento del agente")

class LangChainAgentManager:
    def __init__(self):
        self.api_key = os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            logger.error("‚ùå GEMINI_API_KEY no encontrada.")
            self.llm = None
        else:
            model_name = os.getenv("GEMINI_MODEL", "gemini-2.0-flash")
            self.llm = ChatGoogleGenerativeAI(
                model=model_name,
                google_api_key=self.api_key,
                temperature=0.7,
                max_retries=3
            )
        
        self.histories: Dict[str, List[Any]] = {}

    def get_patient_history(self, patient_id: str) -> List[Any]:
        if patient_id not in self.histories:
            self.histories[patient_id] = []
        return self.histories[patient_id]

    def _get_demo_fallback(self, agent_type: str) -> str:
        """Devuelve una respuesta de alta calidad cuando hay problemas con el servicio de IA."""
        logger.warning(f"üî¶ MODO SEGURO: Interrupci√≥n en el servicio de IA. Usando demo para: {agent_type}")
        
        demos = {
            "risk": "{\"risk_level\": \"high\", \"risk_found\": true, \"explanation\": \"(MODO SEGURO) Detectada crisis de dolor referida (EVA > 7) y posible disnea. Requiere evaluaci√≥n m√©dica inmediata.\"}",
            "soap": (
                "(MODO SEGURO: Interrupci√≥n temporal del servicio de IA)\n\n"
                "S: Paciente reporta dolor lumbar EVA 8/10 refractario a rescates. Refiere n√°useas post-toma.\n"
                "O: Facies de dolor, limitaci√≥n funcional. No signos de depresi√≥n respiratoria.\n"
                "A: Dolor oncol√≥gico mal controlado con posible toxicidad gastrointestinal a opioides.\n"
                "P: 1. Rotaci√≥n de opioide o ajuste de dosis. 2. A√±adir antiem√©tico reglado. 3. Re-evaluar en 24h."
            ),
            "psycho": (
                "(MODO SEGURO: Interrupci√≥n temporal del servicio de IA)\n\n"
                "Estimado/a paciente:\n\n"
                "Entendemos que controlar el dolor y las n√°useas es prioritario para tu bienestar.\n"
                "IMPORTANTE PARA HOY:\n"
                "1. Toma el medicamento para las n√°useas 30 min antes del analg√©sico.\n"
                "2. Registra en tu libreta a qu√© hora aparece el dolor m√°s fuerte.\n\n"
                "Estamos ajustando tu tratamiento para que te sientas mejor. No dudes en llamar si hay cambios."
            ),
            "symptoms": "{\"pain\": 0.8, \"fatigue\": 0.6, \"nausea\": 0.3, \"anxiety\": 0.5, \"depression\": 0.2, \"insomnia\": 0.9}",
            "chat": "Hola. Detecto que el servicio de IA est√° saturado moment√°neamente (Error 429). \n\nNo obstante, aqu√≠ tienes una respuesta basada en protocolos est√°ndar (MODO DEMO):\n\nPara el dolor irruptivo oncol√≥gico, se recomienda utilizar fentanilo transmucoso o intranasal, ajustando la dosis seg√∫n la tolerancia previa a opioides. Es crucial re-evaluar la eficacia a los 15-30 minutos.\n\n(Fuente: Gu√≠a SEOM Dolor Oncol√≥gico - Respuesta Simulada)"
        }
        return demos.get(agent_type, "Servicio temporalmente no disponible.")

    def chat_agent(self, query: str, patient_context: str = "") -> Dict[str, Any]:
        """Agente de Chat Cl√≠nico con RAG (Consultas a Gu√≠as)"""
        # Fallback inmediato si no hay cliente (api key missing)
        if not self.llm:
             return {"answer": self._get_demo_fallback("chat"), "sources": ["Demo_Mode.pdf"]}

        # 1. Recuperar info relevante de RAG
        # rag_service ya est√° importado arriba
        try:
            rag_data = rag_service.query_expert(query)
            rag_context = rag_data.get("context", "")
            sources = rag_data.get("sources", [])
        except Exception as e:
            logger.error(f"Error RAG: {e}")
            rag_context = ""
            sources = []

        # 2. Construir Prompt
        system_instruction = (
            "Eres un Asistente Cl√≠nico Inteligente para Onc√≥logos. Tu objetivo es responder preguntas m√©dicas de forma precisa.\n"
            "INSTRUCCIONES:\n"
            "1. Basa tu respuesta PRINCIPALMENTE en la 'INFORMACI√ìN DE REFERENCIA (RAG)' proporcionada abajo.\n"
            "2. Si la informaci√≥n est√° en el contexto RAG, cita expl√≠citamente que sale de ah√≠ (ej: 'Seg√∫n el documento [Nombre]...').\n"
            "3. Si la respuesta no est√° en el RAG, usa tu conocimiento general m√©dico, PERO avisa claramente: 'Nota: Esta informaci√≥n proviene de mi conocimiento general, no de la base documental local.'.\n"
            "4. S√© conciso, profesional y directo."
        )
        
        human_content = f"INFORMACI√ìN DE REFERENCIA (RAG):\n{rag_context}\n\nCONTEXTO DEL PACIENTE:\n{patient_context}\n\nPREGUNTA DEL M√âDICO:\n{query}"

        try:
            messages = [SystemMessage(content=system_instruction), HumanMessage(content=human_content)]
            response = self.llm.invoke(messages)
            return {"answer": response.content, "sources": sources}
        except Exception as e:
            logger.error(f"‚ùå Error en Chat Agent: {e}")
            if "429" in str(e) or "quota" in str(e).lower():
                return {
                    "answer": self._get_demo_fallback("chat"), 
                    "sources": ["Demo_Mode.pdf (Fallback por Rate Limit)"]
                }
            return {"answer": "Hubo un error procesando tu consulta con el asistente.", "sources": []}
    def analyze_risk_agent(self, text: str) -> RiskAnalysis:
        """Agente especializado en detecci√≥n de urgencias oncol√≥gicas y psiqui√°tricas."""
        if not self.llm:
            import json
            return RiskAnalysis(**json.loads(self._get_demo_fallback("risk")))

        parser = PydanticOutputParser(pydantic_object=RiskAnalysis)
        system_prompt = (
            "Eres un Triaje Oncol√≥gico experto. Analiza el texto buscando URGENCIAS F√çSICAS O PSICOL√ìGICAS.\n"
            "CRITERIOS DE ALARMA:\n"
            "1. Sepsis (fiebre, tiritona).\n"
            "2. Compresi√≥n Medular (p√©rdida fuerza, incontinencia).\n"
            "3. Hemorragia activa.\n"
            "4. Dolor no controlado (Crisis, EVA > 7).\n"
            "5. Asfixia/Disnea.\n"
            "6. Riesgo Suicida.\n"
            "{format_instructions}"
        )
        prompt = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", "{text}")])
        chain = prompt | self.llm | parser
        
        try:
            return chain.invoke({"text": text, "format_instructions": parser.get_format_instructions()})
        except Exception as e:
            logger.error(f"‚ùå Error en Risk Agent: {e}")
            import json
            return RiskAnalysis(**json.loads(self._get_demo_fallback("risk")))

    def generate_soap_agent(self, patient_id: str, raw_text: str, emotion_metrics: Dict[str, float]) -> str:
        """Agente especializado en notas cl√≠nicas oncol√≥gicas (Dolor, Toxicidad, Emocional)."""
        if not self.llm:
            return self._get_demo_fallback("soap")

        metrics_str = ", ".join([f"{k}: {v:.2f}" for k, v in emotion_metrics.items()])
        system_instruction = (
            "Eres un Onc√≥logo experto. Genera una nota S.O.A.P.\n"
            "- S (Subjetivo): S√≠ntomas reportados (dolor, fatiga, n√°useas).\n"
            "- O (Objetivo): M√©tricas emocionales y observaciones cl√≠nicas.\n"
            "- A (An√°lisis): Juicio cl√≠nico integrando estado f√≠sico y an√≠mico.\n"
            "- P (Plan): Ajustes de tratamiento, pruebas o soporte."
        )
        human_content = f"DATOS EMOCIONALES: {metrics_str}\nRELATO DEL PACIENTE: \"{raw_text}\"\nGENERA EL SOAP:"

        try:
            messages = [SystemMessage(content=system_instruction), HumanMessage(content=human_content)]
            response = self.llm.invoke(messages)
            return response.content
        except Exception as e:
            logger.error(f"‚ùå Error en SOAP Agent: {e}")
            return self._get_demo_fallback("soap")

    def generate_psychoeducation_agent(self, patient_id: str, soap_plan: str, emotion_metrics: Dict[str, float]) -> str:
        """Agente de Educaci√≥n al Paciente (Adherencia y Manejo de S√≠ntomas)."""
        if not self.llm:
            return self._get_demo_fallback("psycho")

        metrics_str = ", ".join([f"{k}: {v:.2f}" for k, v in emotion_metrics.items()])
        system_instruction = (
            "Eres un experto en Educaci√≥n al Paciente Oncol√≥gico. Convierte el plan m√©dico en instrucciones claras, emp√°ticas y pr√°cticas para el paciente.\n"
            "Usa lenguaje sencillo. Enf√≥cate en qu√© hacer en casa (autocuidado)."
        )
        human_content = f"M√©tricas Emocionales: {metrics_str}\nPlan M√©dico: {soap_plan}\nGENERA CORREO EDUCATIVO:"

        try:
            messages = [SystemMessage(content=system_instruction), HumanMessage(content=human_content)]
            response = self.llm.invoke(messages)
            return response.content
        except Exception as e:
            logger.error(f"‚ùå Error en Psycho Education Agent: {e}")
            return self._get_demo_fallback("psycho")

    def extract_symptoms_agent(self, text: str) -> Dict[str, float]:
        """Agente de Extracci√≥n de S√≠ntomas (Escala ESAS Estimada)."""
        import json
        
        if not self.llm:
            return json.loads(self._get_demo_fallback("symptoms"))

        # Definimos el esquema de salida esperado
        class SymptomScores(BaseModel):
            pain: float = Field(description="Intensidad de dolor (0.0 a 1.0)")
            anxiety: float = Field(description="Nivel de ansiedad/nerviosismo (0.0 a 1.0)")
            fatigue: float = Field(description="Nivel de cansancio/fatiga (0.0 a 1.0)")
            nausea: float = Field(description="Intensidad de n√°useas/v√≥mitos (0.0 a 1.0)")
            depression: float = Field(description="Nivel de tristeza/decaimiento (0.0 a 1.0)")
            insomnia: float = Field(description="Problemas de sue√±o (0.0 a 1.0)")
        
        parser = PydanticOutputParser(pydantic_object=SymptomScores)
        
        # RAG - Dynamic Few-Shot Prompting (Active Learning)
        # Buscar correcciones pasadas similares para guiar al modelo
        past_corrections = rag_service.find_similar_feedback(text)
        learning_context = ""
        
        if past_corrections:
            learning_context += "\n\nAPRENDIZAJE PREVIO (Casos similares corregidos por expertos):\n"
            for case in past_corrections:
                learning_context += f"- Texto: '{case['text']}' -> Correcci√≥n Experta: {case['correction']}\n"
            learning_context += "USA ESTOS EJEMPLOS PARA CALIBRAR TU PREDICCI√ìN ACTUAL.\n"

        system_instruction = (
            "Eres un experto en Cuidados Paliativos y Oncolog√≠a.\n"
            "Analiza el texto del paciente y estima la intensidad de los s√≠ntomas seg√∫n la Escala de Edmonton (ESAS).\n"
            "Asigna un valor de 0.0 (ausente) a 1.0 (m√°xima intensidad) para cada s√≠ntoma bas√°ndote en el lenguaje usado.\n"
            "Si un s√≠ntoma no se menciona, asigna 0.0."
            "{learning_context}"
            "{format_instructions}"
        )

        prompt = ChatPromptTemplate.from_messages([("system", system_instruction), ("human", "{text}")])
        chain = prompt | self.llm | parser

        try:
            result = chain.invoke({
                "text": text, 
                "format_instructions": parser.get_format_instructions(),
                "learning_context": learning_context
            })
            return result.dict()
        except Exception as e:
            logger.error(f"‚ùå Error en Symptom Extraction Agent: {e}")
            # Fallback a demo en caso de error (e.g. Rate Limit)
            return json.loads(self._get_demo_fallback("symptoms"))



# Instancia global
langchain_agent = LangChainAgentManager()
